![F0E3307D-183C-480E-A259-AA972AEA5EA6](https://github.com/user-attachments/assets/14cab13e-2477-471a-8735-61f213ad352d)
# Knowledge DistillationğŸ§ ğŸ‘¨ğŸ»â€ğŸ«
## Overview
This repository contains the implementation of a knowledge distillation framework that transfers knowledge from simpler, non-neural models to complex neural models. Additionally, the project explores multi-teacher and self-distillation techniques to further enhance model performance and efficiency.

## ğŸ¤– What is Knowledge Distillation?
Knowledge distillation is a technique where a "student" model is trained to replicate the behavior of a "teacher" model. The teacher model is typically a larger or more complex model, but in this project, we also explore scenarios where the teacher models are non-neural models (e.g., decision trees, SVMs). The goal is to leverage the simplicity and interpretability of non-neural models to guide the training of more complex neural networks.

## ğŸŒŸ Project Features
ğŸ”„ **Non-Neural to Neural Knowledge Distillation:** Transfer knowledge from non-neural models (e.g., Decision Trees, SVMs) to neural networks.

ğŸ‘©â€ğŸ« **Multi-Teacher Distillation:** Utilize multiple teacher models to enhance the student model's learning process.

ğŸ” **Self-Distillation:** Implement self-distillation where the student model itself serves as a teacher for its subsequent iterations.

ğŸ“Š **Comprehensive Evaluation:** Evaluate the effectiveness of the distillation process across various datasets and model architectures.

## ğŸš€ The project
ğŸ““ Have fun reading my notebook, with the markdowns to acconpany your travel in the realm of Knowledge distillation

## ğŸ¤ Contributing
I welcome contributions to this project! If you have suggestions or find bugs, please contact me on my email. You can find it in my bio
