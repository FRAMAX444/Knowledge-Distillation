![F0E3307D-183C-480E-A259-AA972AEA5EA6](https://github.com/user-attachments/assets/14cab13e-2477-471a-8735-61f213ad352d)
# Knowledge Distillation🧠👨🏻‍🏫
## Overview
This repository contains the implementation of a knowledge distillation framework that transfers knowledge from simpler, non-neural models to complex neural models. Additionally, the project explores multi-teacher and self-distillation techniques to further enhance model performance and efficiency.

## 🤖 What is Knowledge Distillation?
Knowledge distillation is a technique where a "student" model is trained to replicate the behavior of a "teacher" model. The teacher model is typically a larger or more complex model, but in this project, we also explore scenarios where the teacher models are non-neural models (e.g., decision trees, SVMs). The goal is to leverage the simplicity and interpretability of non-neural models to guide the training of more complex neural networks.

## 🌟 Project Features
🔄 **Non-Neural to Neural Knowledge Distillation:** Transfer knowledge from non-neural models (e.g., Decision Trees, SVMs) to neural networks.

👩‍🏫 **Multi-Teacher Distillation:** Utilize multiple teacher models to enhance the student model's learning process.

🔁 **Self-Distillation:** Implement self-distillation where the student model itself serves as a teacher for its subsequent iterations.

📊 **Comprehensive Evaluation:** Evaluate the effectiveness of the distillation process across various datasets and model architectures.

## 🚀 The project
📓 Have fun reading my notebook, with the markdowns to acconpany your travel in the realm of Knowledge distillation

## 🤝 Contributing
I welcome contributions to this project! If you have suggestions or find bugs, please contact me on my email. You can find it in my bio
