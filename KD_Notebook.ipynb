{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJiPElLUN_Hs"
      },
      "source": [
        "# **Knowledge Distillation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2_p7KtOFh6"
      },
      "source": [
        "Francesco Marrocco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z0Y6hQDOMFD"
      },
      "source": [
        "Knowldege Distillation (KD) è una tecnica utilizzata con lo scopo di rendere Deep Neural Networks più leggere e pratiche da addestrare senza perdere troppa precisione. L'idea di fondo è di permettere a un modello più piccolo di addestrarsi non solo sui dati del training set ma anche su un modello più grande già addestrato.$^1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um8Y5oU-YO_-"
      },
      "source": [
        "$1)$ In realtà esistono vari schemi di distillazione per rete neurale e non tutti richedono la presenza di un modello grande e uno piccolo per l'addestramento. In particolare si possono distinguere tre possibilità:\n",
        "\n",
        "\n",
        "1.   *Offline Distillation* quella che segue la prima spiegazione fornita di KD, ovvero un modello profondo già addestrato e uno più leggero che si aggiorna per minimizzare la seguente loss\n",
        "2.   *Online Distillation*\n",
        "3.   *Self Distillation*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aRBqq8shmKW5"
      },
      "outputs": [],
      "source": [
        "trained_models = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSCb7TYoQvYH"
      },
      "source": [
        "### **Import dependencies and helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0mPsBOSMlPc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pglvyw4cP-dl"
      },
      "outputs": [],
      "source": [
        "def del_all():\n",
        "    # Get a list of all .pth files in the current directory\n",
        "    pth_files = glob.glob('*.pth')\n",
        "\n",
        "    # Loop through the list and delete each file\n",
        "    for file in pth_files:\n",
        "        try:\n",
        "            os.remove(file)\n",
        "            print(f'Deleted: {file}')\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found: {file}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arXTt27qcqDJ"
      },
      "source": [
        "### **Teacher: Non-Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RgTqFt1lPzME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6438963-2d0f-4b49-dd57-8da777270521"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7849fc1a8530>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_HCkyOGpPWva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0059216f-aed1-4f96-c8b1-6f586ea92a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# 1. Train the Non-Neural Teacher Model\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "teacher_model1 = DecisionTreeClassifier()\n",
        "teacher_model1.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate teacher model accuracy\n",
        "teacher_accuracy = 100 * teacher_model1.score(X_test, y_test)\n",
        "print(f'Teacher Model Accuracy: {teacher_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VB62oKB2PdHJ"
      },
      "outputs": [],
      "source": [
        "# 2. Extract Teacher Predictions\n",
        "teacher_logits_train = teacher_model1.predict_proba(X_train)\n",
        "teacher_logits_test = teacher_model1.predict_proba(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "teacher_logits_train_tensor = torch.tensor(teacher_logits_train, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor, teacher_logits_train_tensor)\n",
        "train_loader_iris = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wKZ7Xcy6PgHw"
      },
      "outputs": [],
      "source": [
        "# 3. Define the Student Neural Network\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 10)\n",
        "        self.fc2 = nn.Linear(10, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "student_model_no_kd_non_neural_0 = StudentNet()\n",
        "student_model_with_kd_non_neural_0 = StudentNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7tjZ-XDgPoNB"
      },
      "outputs": [],
      "source": [
        "# 4. Distillation Loss Function\n",
        "def distillation_loss(student_logits, teacher_logits, targets, alpha=0.5, temperature=2.0):\n",
        "    soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=1)\n",
        "    soft_student_logits = nn.functional.log_softmax(student_logits / temperature, dim=1)\n",
        "    distillation_loss = nn.functional.kl_div(soft_student_logits, soft_targets, reduction='batchmean') * (temperature ** 2)\n",
        "    hard_loss = nn.functional.cross_entropy(student_logits, targets)\n",
        "    return alpha * distillation_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "# Helper function to train the student model\n",
        "def train_student_model1(model, use_kd=False, num_epochs=50):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets, teacher_logits in train_loader_iris:\n",
        "            optimizer.zero_grad()\n",
        "            student_logits = model(inputs)\n",
        "            if use_kd:\n",
        "                loss = distillation_loss(student_logits, teacher_logits, targets)\n",
        "            else:\n",
        "                loss = nn.functional.cross_entropy(student_logits, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader_iris.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s0GK7BEy8Zwk"
      },
      "outputs": [],
      "source": [
        "def test1(student_model_no_kd, X_test, y_test):\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "        student_logits_test_no_kd = student_model_no_kd(X_test_tensor)\n",
        "        student_predictions_no_kd = torch.argmax(student_logits_test_no_kd, dim=1).numpy()\n",
        "        student_accuracy_no_kd = 100 * (student_predictions_no_kd == y_test).mean()\n",
        "    return student_accuracy_no_kd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DuLqF-ikPsVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38eb6366-2284-4e59-e17e-2495aa9d6651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model Accuracy without KD: 76.67%\n"
          ]
        }
      ],
      "source": [
        "# 5. Train the Student Model without KD\n",
        "train_student_model1(student_model_no_kd_non_neural_0, use_kd=False)\n",
        "\n",
        "student_accuracy_no_kd_0 = test1(student_model_no_kd_non_neural_0, X_test, y_test)\n",
        "\n",
        "torch.save(student_model_no_kd_non_neural_0.state_dict(), 'student_model_no_kd_non_neural_0.pth')\n",
        "\n",
        "print(f\"Student Model Accuracy without KD: {student_accuracy_no_kd_0:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y5w8ImxSPweD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c180b632-cc28-4d0e-c3ac-b95b46b17715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model Accuracy with KD: 76.67%\n"
          ]
        }
      ],
      "source": [
        "# 6. Train the Student Model with KD\n",
        "train_student_model1(student_model_with_kd_non_neural_0, use_kd=True, num_epochs=50)\n",
        "\n",
        "student_accuracy_with_kd_0 = test1(student_model_with_kd_non_neural_0, X_test, y_test)\n",
        "\n",
        "torch.save(student_model_with_kd_non_neural_0.state_dict(), 'student_model_with_kd_non_neural_0.pth')\n",
        "\n",
        "print(f'Student Model Accuracy with KD: {student_accuracy_with_kd_0:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kWwPwuRDomMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e74884-58a9-4bf5-b21d-a10943e66714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model without KD Accuracy: 85.28%\n",
            "Student Model with KD Accuracy: 86.50%\n"
          ]
        }
      ],
      "source": [
        "student_no_kd = 0\n",
        "student_with_kd = 0\n",
        "\n",
        "num_iterations = 101\n",
        "\n",
        "if not trained_models:\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        torch.manual_seed(_)\n",
        "        student_model_no_kd = StudentNet()\n",
        "        student_model_with_kd = StudentNet()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=_)\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        train_student_model1(student_model_no_kd, use_kd=False)\n",
        "        student_accuracy_no_kd = test1(student_model_no_kd, X_test, y_test)\n",
        "        student_no_kd += student_accuracy_no_kd\n",
        "        torch.save(student_model_no_kd.state_dict(), f'student_model_no_kd_non_neural_{_}.pth')\n",
        "\n",
        "        train_student_model1(student_model_with_kd, use_kd=True)\n",
        "        student_accuracy_with_kd = test1(student_model_with_kd, X_test, y_test)\n",
        "        student_with_kd += student_accuracy_with_kd\n",
        "        torch.save(student_model_with_kd.state_dict(), f'student_model_with_kd_non_neural_{_}.pth')\n",
        "\n",
        "        with zipfile.ZipFile('first_MLP_models.zip', 'a') as zipf:\n",
        "            zipf.write(f'student_model_no_kd_non_neural_{_}.pth')\n",
        "            zipf.write(f'student_model_with_kd_non_neural_{_}.pth')\n",
        "\n",
        "    with zipfile.ZipFile('non_neural_models.zip', 'w') as zipf:\n",
        "        for i in range(num_iterations):\n",
        "            zipf.write(f'student_model_no_kd_non_neural_{i}.pth')\n",
        "            zipf.write(f'student_model_with_kd_non_neural_{i}.pth')\n",
        "\n",
        "    # Download the zip file\n",
        "    files.download('non_neural_models.zip')\n",
        "\n",
        "else:\n",
        "\n",
        "    with zipfile.ZipFile('non_neural_models.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    for i in range(num_iterations):\n",
        "        torch.manual_seed(i)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=i)\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        student_model_no_kd = StudentNet()\n",
        "        student_model_no_kd.load_state_dict(torch.load(f'student_model_no_kd_non_neural_{i}.pth'))\n",
        "        acc1 = test1(student_model_no_kd, X_test, y_test)\n",
        "        student_no_kd += acc1\n",
        "\n",
        "        student_model_with_kd = StudentNet()\n",
        "        student_model_with_kd.load_state_dict(torch.load(f'student_model_with_kd_non_neural_{i}.pth'))\n",
        "        acc2 = test1(student_model_with_kd, X_test, y_test)\n",
        "        student_with_kd += acc2\n",
        "\n",
        "student_no_kd /= num_iterations\n",
        "student_with_kd /= num_iterations\n",
        "print(f'Student Model without KD Accuracy: {student_no_kd:.2f}%')\n",
        "print(f'Student Model with KD Accuracy: {student_with_kd:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nof5t3kelMFv"
      },
      "source": [
        "A questo punto posso eliminare le reti neurali salvate (o anche no se si vuole continuare a giocarci un po' :) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "U-Y8y_znBenr"
      },
      "outputs": [],
      "source": [
        "# Delete the files\n",
        "for i in range(num_iterations):\n",
        "    try:\n",
        "        os.remove(f'student_model_no_kd_non_neural_{i}.pth')\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        os.remove(f'student_model_with_kd_non_neural_{i}.pth')\n",
        "    except FileNotFoundError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90gGp8tQ9ty"
      },
      "source": [
        "### **Teacher: Small MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mCXTbe1WKGmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769e6c2e-bd83-4fa2-deea-fbdb26d71d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "v8uHyPSrH7k0"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the FashionMNIST dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cLSxVgObH4iy"
      },
      "outputs": [],
      "source": [
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XzcXV0knJY_W"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F2DaV-7WJb3D"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / len(test_loader.dataset)\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mjVWy9lZKW0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9cc94f0-bd0e-48ee-a19a-a17b061f6056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 87.28%\n"
          ]
        }
      ],
      "source": [
        "if not trained_models:\n",
        "    torch.manual_seed(42)\n",
        "    teacher_model = TeacherModel().to(device)\n",
        "    teacher_model = train(teacher_model, epochs = 10)\n",
        "    teacher_accuracy = test_model(teacher_model, test_loader)\n",
        "    torch.save(teacher_model.state_dict(), f'teacher_model_1_NN_42.pth')\n",
        "    with zipfile.ZipFile('first_MLP_models.zip', 'a') as zipf:\n",
        "        zipf.write(f'teacher_model_1_NN_42.pth')\n",
        "else:\n",
        "    teacher_model = TeacherModel().to(device)\n",
        "    # Open the zip file\n",
        "    with zipfile.ZipFile('first_MLP_models.zip', 'r') as zip_ref:\n",
        "        # Extract the specified file\n",
        "        zip_ref.extract('teacher_model_1_NN_42.pth')\n",
        "    teacher_model.load_state_dict(torch.load(f'teacher_model_1_NN_42.pth'))\n",
        "    teacher_accuracy = test_model(teacher_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TqIZam8LLzRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb9fe7f-078e-4203-d649-c74ddd7491cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 86.00%\n"
          ]
        }
      ],
      "source": [
        "if not trained_models:\n",
        "    torch.manual_seed(42)\n",
        "    student_model = StudentModel().to(device)\n",
        "    student_model = train(student_model, epochs = 10)\n",
        "    student_accuracy = test_model(student_model, test_loader)\n",
        "    torch.save(student_model.state_dict(), f'smaller_model_1_NN_42.pth')\n",
        "    with zipfile.ZipFile('first_MLP_models.zip', 'a') as zipf:\n",
        "        zipf.write(f'smaller_model_1_NN_42.pth')\n",
        "\n",
        "\n",
        "else:\n",
        "    student_model = StudentModel().to(device)\n",
        "    # Open the zip file\n",
        "    with zipfile.ZipFile('first_MLP_models.zip', 'r') as zip_ref:\n",
        "        # Extract the specified file\n",
        "        zip_ref.extract('smaller_model_1_NN_42.pth')\n",
        "    student_model.load_state_dict(torch.load(f'smaller_model_1_NN_42.pth'))\n",
        "    student_accuracy = test_model(student_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xUKnr0uMIB9y"
      },
      "outputs": [],
      "source": [
        "def train_student_model(teacher, student, train_loader, epochs = 10, alpha=0.5, T=2.0):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train()  # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            # Soften the teacher logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = nn.KLDivLoss()(soft_prob, soft_targets) * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = alpha * soft_targets_loss + (1 - alpha) * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    return student"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2M_4ow1abdt"
      },
      "source": [
        "https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_CgpTheBMdNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874307c8-2453-4151-b76b-8b103280859d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 85.95%\n"
          ]
        }
      ],
      "source": [
        "if not trained_models:\n",
        "    torch.manual_seed(42)\n",
        "    student_model_distillated = StudentModel().to(device)\n",
        "    student_model_distillated = train_student_model(teacher_model, student_model_distillated, train_loader)\n",
        "    student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "    torch.save(student_model_distillated.state_dict(), f'student_model_1_NN_42.pth')\n",
        "    with zipfile.ZipFile('first_MLP_models.zip', 'a') as zipf:\n",
        "        zipf.write(f'student_model_1_NN_42.pth')\n",
        "else:\n",
        "    student_model_distillated = StudentModel().to(device)\n",
        "    # Open the zip file\n",
        "    with zipfile.ZipFile('first_MLP_models.zip', 'r') as zip_ref:\n",
        "        # Extract the specified file\n",
        "        zip_ref.extract('student_model_1_NN_42.pth')\n",
        "    student_model_distillated.load_state_dict(torch.load(f'student_model_1_NN_42.pth'))\n",
        "    student_distillated_accuracy = test_model(student_model_distillated, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HP74UR8GxADs"
      },
      "outputs": [],
      "source": [
        "if not trained_models: files.download('first_MLP_models.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KG-Zj8qDTkwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7cb1195-c101-4f66-aba7-9d902bd81321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher NN: 575,050\n",
            "Smaller NN: 25,450\n",
            "Smaller NN distillated: 25,450\n"
          ]
        }
      ],
      "source": [
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in teacher_model.parameters()))\n",
        "print(f\"Teacher NN: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in student_model.parameters()))\n",
        "print(f\"Smaller NN: {total_params_light}\")\n",
        "total_params_dist = \"{:,}\".format(sum(p.numel() for p in student_model_distillated.parameters()))\n",
        "print(f\"Smaller NN distillated: {total_params_dist}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "x8rlMkGWUIIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e1db91-a3b0-4933-cf3a-23b4734c8cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher NN accuracy: 87.28%\n",
            "Smaller NN accuracy: 86.00%\n",
            "Smaller NN distillated accuracy: 85.95%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Teacher NN accuracy: {teacher_accuracy:.2f}%\")\n",
        "print(f\"Smaller NN accuracy: {student_accuracy:.2f}%\")\n",
        "print(f\"Smaller NN distillated accuracy: {student_distillated_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pyXmGyRqT8eu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ded0e3-df47-4af6-8609-4a755ab869ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 87.18%\n",
            "Test Accuracy: 86.66%\n",
            "Test Accuracy: 87.03%\n",
            "Test Accuracy: 86.54%\n",
            "Test Accuracy: 85.67%\n",
            "Test Accuracy: 86.88%\n",
            "Test Accuracy: 86.74%\n",
            "Test Accuracy: 86.80%\n",
            "Test Accuracy: 86.47%\n",
            "Test Accuracy: 85.13%\n",
            "Test Accuracy: 87.05%\n",
            "Test Accuracy: 86.64%\n",
            "Test Accuracy: 87.05%\n",
            "Test Accuracy: 86.23%\n",
            "Test Accuracy: 85.92%\n",
            "Test Accuracy: 86.88%\n",
            "Test Accuracy: 86.94%\n",
            "Test Accuracy: 87.15%\n",
            "Test Accuracy: 86.49%\n",
            "Test Accuracy: 85.44%\n",
            "Test Accuracy: 87.06%\n",
            "Test Accuracy: 86.55%\n",
            "Test Accuracy: 86.80%\n",
            "Test Accuracy: 86.15%\n",
            "Test Accuracy: 86.09%\n",
            "Test Accuracy: 86.77%\n",
            "Test Accuracy: 86.41%\n",
            "Test Accuracy: 86.49%\n",
            "Test Accuracy: 86.26%\n",
            "Test Accuracy: 85.79%\n",
            "Test Accuracy: 87.28%\n",
            "Test Accuracy: 87.05%\n",
            "Test Accuracy: 87.13%\n",
            "Test Accuracy: 85.69%\n",
            "Test Accuracy: 85.79%\n",
            "Test Accuracy: 87.33%\n",
            "Test Accuracy: 86.84%\n",
            "Test Accuracy: 86.64%\n",
            "Test Accuracy: 86.13%\n",
            "Test Accuracy: 86.50%\n",
            "Test Accuracy: 86.98%\n",
            "Test Accuracy: 86.48%\n",
            "Test Accuracy: 86.73%\n",
            "Test Accuracy: 86.37%\n",
            "Test Accuracy: 86.63%\n",
            "[[87.18, ('alpha', 0.25), ('temperature', 1.0)], [86.66, ('alpha', 0.25), ('temperature', 1.0)], [87.03, ('alpha', 0.25), ('temperature', 1.0)], [86.54, ('alpha', 0.25), ('temperature', 1.0)], [85.67, ('alpha', 0.25), ('temperature', 1.0)], [86.88, ('alpha', 0.25), ('temperature', 2.0)], [86.74, ('alpha', 0.25), ('temperature', 2.0)], [86.8, ('alpha', 0.25), ('temperature', 2.0)], [86.47, ('alpha', 0.25), ('temperature', 2.0)], [85.13, ('alpha', 0.25), ('temperature', 2.0)], [87.05, ('alpha', 0.25), ('temperature', 3.0)], [86.64, ('alpha', 0.25), ('temperature', 3.0)], [87.05, ('alpha', 0.25), ('temperature', 3.0)], [86.23, ('alpha', 0.25), ('temperature', 3.0)], [85.92, ('alpha', 0.25), ('temperature', 3.0)], [86.88, ('alpha', 0.5), ('temperature', 1.0)], [86.94, ('alpha', 0.5), ('temperature', 1.0)], [87.15, ('alpha', 0.5), ('temperature', 1.0)], [86.49, ('alpha', 0.5), ('temperature', 1.0)], [85.44, ('alpha', 0.5), ('temperature', 1.0)], [87.06, ('alpha', 0.5), ('temperature', 2.0)], [86.55, ('alpha', 0.5), ('temperature', 2.0)], [86.8, ('alpha', 0.5), ('temperature', 2.0)], [86.15, ('alpha', 0.5), ('temperature', 2.0)], [86.09, ('alpha', 0.5), ('temperature', 2.0)], [86.77, ('alpha', 0.5), ('temperature', 3.0)], [86.41, ('alpha', 0.5), ('temperature', 3.0)], [86.49, ('alpha', 0.5), ('temperature', 3.0)], [86.26, ('alpha', 0.5), ('temperature', 3.0)], [85.79, ('alpha', 0.5), ('temperature', 3.0)], [87.28, ('alpha', 0.75), ('temperature', 1.0)], [87.05, ('alpha', 0.75), ('temperature', 1.0)], [87.13, ('alpha', 0.75), ('temperature', 1.0)], [85.69, ('alpha', 0.75), ('temperature', 1.0)], [85.79, ('alpha', 0.75), ('temperature', 1.0)], [87.33, ('alpha', 0.75), ('temperature', 2.0)], [86.84, ('alpha', 0.75), ('temperature', 2.0)], [86.64, ('alpha', 0.75), ('temperature', 2.0)], [86.13, ('alpha', 0.75), ('temperature', 2.0)], [86.5, ('alpha', 0.75), ('temperature', 2.0)], [86.98, ('alpha', 0.75), ('temperature', 3.0)], [86.48, ('alpha', 0.75), ('temperature', 3.0)], [86.73, ('alpha', 0.75), ('temperature', 3.0)], [86.37, ('alpha', 0.75), ('temperature', 3.0)], [86.63, ('alpha', 0.75), ('temperature', 3.0)]]\n"
          ]
        }
      ],
      "source": [
        "#@title *Finding good $\\alpha$ and  $T$*\n",
        "\n",
        "# Initialize accumulators for total accuracy\n",
        "total_acc = 0\n",
        "total_acc_dist = 0\n",
        "student_precision = []\n",
        "# Define number of iterations\n",
        "num_iterations = 5\n",
        "\n",
        "# Experiment with different alpha and temperature values\n",
        "alpha_values = [0.25, 0.5, 0.75]\n",
        "temperature_values = [1.0, 2.0, 3.0]\n",
        "\n",
        "if not trained_models:\n",
        "    for alpha in alpha_values:\n",
        "        for temperature in temperature_values:\n",
        "\n",
        "            for i in range(num_iterations):\n",
        "                # Reset the random seed for reproducibility\n",
        "                torch.manual_seed(i)\n",
        "\n",
        "                # Initialize and train the student model using distillation\n",
        "                student_model_distillated = StudentModel().to(device)\n",
        "                student_model_distillated = train_student_model(teacher_model, student_model_distillated, train_loader, epochs = 10, alpha=alpha, T=temperature)\n",
        "                torch.save(student_model_distillated.state_dict(), f'model_{alpha}_{temperature}_{i}.pth')\n",
        "                with zipfile.ZipFile('finding_a_T.zip', 'a') as zipf:\n",
        "                    zipf.write(f'model_{alpha}_{temperature}_{i}.pth')\n",
        "                # Test the student model and accumulate accuracy\n",
        "                student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "                student_precision.append([student_distillated_accuracy, (\"alpha\",alpha), (\"temperature\", temperature)])\n",
        "else:\n",
        "    for alpha in alpha_values:\n",
        "        for temperature in temperature_values:\n",
        "            for i in range(num_iterations):\n",
        "                torch.manual_seed(i)\n",
        "                student_model_distillated = StudentModel().to(device)\n",
        "                with zipfile.ZipFile('finding_a_T.zip', 'r') as zip_ref:\n",
        "                    # Extract the specified file\n",
        "                    zip_ref.extract(f'model_{alpha}_{temperature}_{i}.pth')\n",
        "                student_model_distillated.load_state_dict(torch.load(f'model_{alpha}_{temperature}_{i}.pth'))\n",
        "                student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "                student_precision.append([student_distillated_accuracy, (\"alpha\",alpha), (\"temperature\", temperature)])\n",
        "\n",
        "if not trained_models: files.download('finding_a_T.zip')\n",
        "print(student_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GXM8iBdX7J6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141366bd-45ba-4588-9e83-5aa31e6f6476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 87.06%\n",
            "Test Accuracy: 86.79%\n",
            "Test Accuracy: 86.78%\n",
            "Test Accuracy: 86.43%\n",
            "Test Accuracy: 85.34%\n",
            "432.4\n"
          ]
        }
      ],
      "source": [
        "# Initialize accumulators for total accuracy\n",
        "total_acc = 0\n",
        "# Define number of iterations\n",
        "num_iterations = 5\n",
        "\n",
        "if not trained_models:\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Reset the random seed for reproducibility\n",
        "        torch.manual_seed(i)\n",
        "\n",
        "        # Initialize and train the student model using distillation\n",
        "        student_model = StudentModel().to(device)\n",
        "        student_model = train(student_model, epochs = 10)\n",
        "        torch.save(student_model.state_dict(), f'model_{0.0}_{1.0}_{i}.pth')\n",
        "        with zipfile.ZipFile('finding_a_T.zip', 'a') as zipf:\n",
        "            zipf.write(f'model_{0.0}_{1.0}_{i}.pth')\n",
        "        # Test the student model and accumulate accuracy\n",
        "        student_accuracy = test_model(student_model, test_loader)\n",
        "else:\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        torch.manual_seed(i)\n",
        "        student = StudentModel().to(device)\n",
        "        with zipfile.ZipFile('finding_a_T.zip', 'r') as zip_ref:\n",
        "            # Extract the specified file\n",
        "            zip_ref.extract(f'model_{0.0}_{1.0}_{i}.pth')\n",
        "        student.load_state_dict(torch.load(f'model_{0.0}_{1.0}_{i}.pth'))\n",
        "        student_accuracy = test_model(student, test_loader)\n",
        "        total_acc+=student_accuracy\n",
        "    print(total_acc)\n",
        "\n",
        "if not trained_models: files.download('finding_a_T.zip')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = []\n",
        "for i in range(len(student_precision)):\n",
        "    if i%5==0:\n",
        "        res.append(student_precision[i])\n",
        "    else:\n",
        "        res[i//5][0] += student_precision[i][0]\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "qSu3Iqzq6UWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30058968-fb35-4d24-8ae9-ac982062a2e3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[433.08000000000004, ('alpha', 0.25), ('temperature', 1.0)], [432.02, ('alpha', 0.25), ('temperature', 2.0)], [432.89000000000004, ('alpha', 0.25), ('temperature', 3.0)], [432.90000000000003, ('alpha', 0.5), ('temperature', 1.0)], [432.6500000000001, ('alpha', 0.5), ('temperature', 2.0)], [431.72, ('alpha', 0.5), ('temperature', 3.0)], [432.94, ('alpha', 0.75), ('temperature', 1.0)], [433.44, ('alpha', 0.75), ('temperature', 2.0)], [433.19, ('alpha', 0.75), ('temperature', 3.0)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = sorted(res, key=lambda x: x[0], reverse = True)\n",
        "best_accuracy = f'{res[0][0]/5:.2f}%'\n",
        "best_alpha = res[0][1][1]\n",
        "best_temperature = res[0][2][1]\n",
        "print(f'Best accuracy: {best_accuracy} for alpha = {best_alpha} and T = {best_temperature}')\n",
        "print(f'Same dimension NN on same dataset accuracy = {total_acc/5:.2f}%')"
      ],
      "metadata": {
        "id": "3ZMKlM2d8Wn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415d57bc-8ef6-4e88-de73-7c4ad22b1c6c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accuracy: 86.69% for alpha = 0.75 and T = 2.0\n",
            "Same dimension NN on same dataset accuracy = 86.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del_all()"
      ],
      "metadata": {
        "id": "hSU97wX51j6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a21728a-4b6f-4153-c1f5-4d9b936181a4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: model_0.25_1.0_3.pth\n",
            "Deleted: model_0.25_2.0_4.pth\n",
            "Deleted: model_0.5_1.0_0.pth\n",
            "Deleted: model_0.75_3.0_0.pth\n",
            "Deleted: model_0.25_1.0_4.pth\n",
            "Deleted: model_0.0_1.0_0.pth\n",
            "Deleted: model_0.75_1.0_0.pth\n",
            "Deleted: model_0.25_3.0_2.pth\n",
            "Deleted: model_0.25_3.0_1.pth\n",
            "Deleted: model_0.5_2.0_4.pth\n",
            "Deleted: model_0.0_1.0_3.pth\n",
            "Deleted: model_0.75_3.0_2.pth\n",
            "Deleted: model_0.5_3.0_3.pth\n",
            "Deleted: model_0.0_1.0_4.pth\n",
            "Deleted: teacher_model_1_NN_42.pth\n",
            "Deleted: model_0.75_3.0_3.pth\n",
            "Deleted: model_0.75_2.0_0.pth\n",
            "Deleted: student_model_1_NN_42.pth\n",
            "Deleted: model_0.5_3.0_0.pth\n",
            "Deleted: model_0.75_1.0_4.pth\n",
            "Deleted: model_0.5_2.0_3.pth\n",
            "Deleted: model_0.25_2.0_3.pth\n",
            "Deleted: model_0.25_1.0_0.pth\n",
            "Deleted: model_0.25_1.0_1.pth\n",
            "Deleted: model_0.25_1.0_2.pth\n",
            "Deleted: model_0.5_3.0_4.pth\n",
            "Deleted: model_0.0_1.0_2.pth\n",
            "Deleted: model_0.5_1.0_2.pth\n",
            "Deleted: model_0.75_2.0_2.pth\n",
            "Deleted: model_0.5_3.0_2.pth\n",
            "Deleted: model_0.5_2.0_0.pth\n",
            "Deleted: model_0.25_2.0_1.pth\n",
            "Deleted: model_0.5_1.0_3.pth\n",
            "Deleted: model_0.75_3.0_4.pth\n",
            "Deleted: model_0.5_2.0_1.pth\n",
            "Deleted: model_0.25_2.0_0.pth\n",
            "Deleted: model_0.25_2.0_2.pth\n",
            "Deleted: model_0.5_1.0_1.pth\n",
            "Deleted: smaller_model_1_NN_42.pth\n",
            "Deleted: model_0.75_1.0_2.pth\n",
            "Deleted: model_0.75_2.0_3.pth\n",
            "Deleted: model_0.25_3.0_0.pth\n",
            "Deleted: model_0.5_1.0_4.pth\n",
            "Deleted: model_0.0_1.0_1.pth\n",
            "Deleted: model_0.25_3.0_4.pth\n",
            "Deleted: model_0.75_1.0_1.pth\n",
            "Deleted: model_0.75_2.0_1.pth\n",
            "Deleted: model_0.5_2.0_2.pth\n",
            "Deleted: model_0.25_3.0_3.pth\n",
            "Deleted: model_0.75_1.0_3.pth\n",
            "Deleted: model_0.75_3.0_1.pth\n",
            "Deleted: model_0.5_3.0_1.pth\n",
            "Deleted: model_0.75_2.0_4.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quindi ho insegnanti molto precisi e informativi tanto che ...\n",
        "Ma il miglioramento sembra minimo allora sperimento nuove tecniche per cercare di alzare un po' l'accuracy"
      ],
      "metadata": {
        "id": "3ZUrAeEJAsTA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snNgLQ_KJvL5"
      },
      "source": [
        "### ***Multi teacher KD***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "afW-kmnhK9CH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920f3936-960d-45fe-e2b2-cabcd44cca61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 88.27%\n",
            "Test Accuracy: 88.23%\n",
            "Test Accuracy: 87.95%\n",
            "Test Accuracy: 88.32%\n",
            "Test Accuracy: 88.13%\n",
            "Test Accuracy: 88.42%\n",
            "Test Accuracy: 88.61%\n",
            "Test Accuracy: 87.62%\n",
            "Test Accuracy: 88.47%\n",
            "Test Accuracy: 88.47%\n",
            "Test Accuracy: 88.50%\n",
            "Test Accuracy: 88.70%\n",
            "Test Accuracy: 88.30%\n",
            "Test Accuracy: 88.48%\n",
            "Test Accuracy: 88.28%\n",
            "Test Accuracy: 87.50%\n",
            "Test Accuracy: 88.45%\n",
            "Test Accuracy: 88.58%\n",
            "Test Accuracy: 88.09%\n",
            "Test Accuracy: 87.77%\n"
          ]
        }
      ],
      "source": [
        "teachers = []\n",
        "num_teachers = 20\n",
        "if not trained_models:\n",
        "    for i in range(num_teachers):\n",
        "        torch.manual_seed(i)\n",
        "        teacher = TeacherModel().to(device)\n",
        "        teacher = train(teacher, epochs = 10)\n",
        "        teachers.append(teachers)\n",
        "        torch.save(teacher.state_dict(), f'teacher_{i}.pth')\n",
        "        with zipfile.ZipFile(f'MTKD.zip', 'a') as zipf:\n",
        "            zipf.write(f'teacher_{i}.pth')\n",
        "    files.download(f'MTKD.zip')\n",
        "else:\n",
        "    for i in range(num_teachers):\n",
        "        torch.manual_seed(i)\n",
        "        teacher = TeacherModel().to(device)\n",
        "        with zipfile.ZipFile('MTKD.zip', 'r') as zip_ref:\n",
        "            # Extract the specified file\n",
        "            zip_ref.extract(f'teacher_{i}.pth')\n",
        "        teacher.load_state_dict(torch.load(f'teacher_{i}.pth'))\n",
        "        teacher_accuracy = test_model(teacher, test_loader)\n",
        "        teachers.append(teacher)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del_all()"
      ],
      "metadata": {
        "id": "3LS38b9P1mb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fde922-75a9-4dc0-bdb4-37e261f1f2d5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: teacher_7.pth\n",
            "Deleted: teacher_19.pth\n",
            "Deleted: teacher_18.pth\n",
            "Deleted: teacher_0.pth\n",
            "Deleted: teacher_11.pth\n",
            "Deleted: teacher_14.pth\n",
            "Deleted: teacher_12.pth\n",
            "Deleted: teacher_4.pth\n",
            "Deleted: teacher_17.pth\n",
            "Deleted: teacher_3.pth\n",
            "Deleted: teacher_16.pth\n",
            "Deleted: teacher_6.pth\n",
            "Deleted: teacher_1.pth\n",
            "Deleted: teacher_8.pth\n",
            "Deleted: teacher_10.pth\n",
            "Deleted: teacher_15.pth\n",
            "Deleted: teacher_5.pth\n",
            "Deleted: teacher_9.pth\n",
            "Deleted: teacher_13.pth\n",
            "Deleted: teacher_2.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "BGEuXNJPKAYs"
      },
      "outputs": [],
      "source": [
        "def train_student_model_multi_teacher(teachers, student, loader = train_loader, epochs = 10, alpha=0.5, T=2.0, device='cpu'):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "    for teacher in teachers:\n",
        "        teacher.eval()  # Set all teacher models to evaluation mode\n",
        "    student.train()  # Student in training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher models - do not save gradients\n",
        "            teacher_logits = []\n",
        "            with torch.no_grad():\n",
        "                for teacher in teachers:\n",
        "                    teacher_logits.append(teacher(inputs))\n",
        "\n",
        "            # Average the logits from the teacher models\n",
        "            avg_teacher_logits = sum(teacher_logits) / len(teacher_logits)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            # Soften the teacher and student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(avg_teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = nn.KLDivLoss()(soft_prob, soft_targets) * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = alpha * soft_targets_loss + (1 - alpha) * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(loader):.4f}\")\n",
        "\n",
        "    return student\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AVG MTKD $\\alpha=0.5$"
      ],
      "metadata": {
        "id": "h0uunPoBMdc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hrWcdVO8Mfpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d96286-1a52-4d44-8552-18225b50fa47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 87.12%\n",
            "Test Accuracy: 86.85%\n",
            "Test Accuracy: 87.22%\n",
            "Test Accuracy: 86.05%\n",
            "Test Accuracy: 85.72%\n",
            "Test Accuracy: 86.96%\n",
            "Test Accuracy: 87.02%\n",
            "Test Accuracy: 87.05%\n",
            "Test Accuracy: 86.16%\n",
            "Test Accuracy: 85.55%\n",
            "Test Accuracy: 87.21%\n",
            "Test Accuracy: 86.46%\n",
            "Test Accuracy: 87.27%\n",
            "Test Accuracy: 86.79%\n",
            "Test Accuracy: 85.73%\n",
            "Test Accuracy: 86.98%\n",
            "Test Accuracy: 86.76%\n",
            "Test Accuracy: 87.08%\n",
            "Test Accuracy: 86.07%\n",
            "Test Accuracy: 85.98%\n"
          ]
        }
      ],
      "source": [
        "num_teachers = [2, 5, 10, 20]\n",
        "\n",
        "if not trained_models:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            # Reset the random seed for reproducibility\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            student_model_distillated = train_student_model_multi_teacher(teachers[:num], student_model_distillated, epochs = 10, alpha = 0.5, T = 2.0)\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "            torch.save(student_model_distillated.state_dict(), f'MTKD_student_{num}_{i}.pth')\n",
        "            with zipfile.ZipFile('MTKD_students.zip', 'a') as zipf:\n",
        "                zipf.write(f'MTKD_student_{num}_{i}.pth')\n",
        "\n",
        "else:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            with zipfile.ZipFile('MTKD_students.zip', 'r') as zip_ref:\n",
        "                # Extract the specified file\n",
        "                zip_ref.extract(f'MTKD_student_{num}_{i}.pth')\n",
        "            student_model_distillated.load_state_dict(torch.load(f'MTKD_student_{num}_{i}.pth'))\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "\n",
        "if not trained_models: files.download('MTKD_students.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AVG MTKD $\\alpha=0.75$"
      ],
      "metadata": {
        "id": "EYsZhJGOFI8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_teachers = [2, 5, 10, 20]\n",
        "\n",
        "if not trained_models:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            # Reset the random seed for reproducibility\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            student_model_distillated = train_student_model_multi_teacher(teachers[:num], student_model_distillated, epochs = 10, alpha = 0.75, T = 2.0)\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "            torch.save(student_model_distillated.state_dict(), f'MTKD_student075_{num}_{i}.pth')\n",
        "            with zipfile.ZipFile('MTKD_students_075.zip', 'a') as zipf:\n",
        "                zipf.write(f'MTKD_student075_{num}_{i}.pth')\n",
        "\n",
        "else:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            with zipfile.ZipFile('MTKD_students_075.zip', 'r') as zip_ref:\n",
        "                # Extract the specified file\n",
        "                zip_ref.extract(f'MTKD_student075_{num}_{i}.pth')\n",
        "            student_model_distillated.load_state_dict(torch.load(f'MTKD_student075_{num}_{i}.pth'))\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "\n",
        "if not trained_models: files.download('MTKD_students_075.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeozXp7iEYDQ",
        "outputId": "eec4e398-788e-4a5a-ba54-1b362092d38c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 86.79%\n",
            "Test Accuracy: 86.78%\n",
            "Test Accuracy: 86.51%\n",
            "Test Accuracy: 86.79%\n",
            "Test Accuracy: 86.00%\n",
            "Test Accuracy: 86.93%\n",
            "Test Accuracy: 86.71%\n",
            "Test Accuracy: 86.72%\n",
            "Test Accuracy: 86.48%\n",
            "Test Accuracy: 86.22%\n",
            "Test Accuracy: 86.98%\n",
            "Test Accuracy: 86.70%\n",
            "Test Accuracy: 87.00%\n",
            "Test Accuracy: 86.47%\n",
            "Test Accuracy: 86.56%\n",
            "Test Accuracy: 87.09%\n",
            "Test Accuracy: 86.86%\n",
            "Test Accuracy: 87.05%\n",
            "Test Accuracy: 86.86%\n",
            "Test Accuracy: 86.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FJgHKIF4S2Sx"
      },
      "outputs": [],
      "source": [
        "#Multi Teacher KD versione 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "p5OXtjdtS2Sx"
      },
      "outputs": [],
      "source": [
        "def train_student(teachers, student, temperature, alpha, epochs):\n",
        "    dataloader = train_loader\n",
        "    optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "    criterion_ce = nn.CrossEntropyLoss()\n",
        "    criterion_kld = nn.KLDivLoss(reduction='batchmean')\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Get teacher logits and compute KLD loss\n",
        "            kld_loss = 0\n",
        "            for teacher in teachers:\n",
        "                teacher.eval()\n",
        "                with torch.no_grad():\n",
        "                    teacher_logits = teacher(images) / temperature\n",
        "                    teacher_softmax = F.softmax(teacher_logits, dim=1)\n",
        "                    student_logits = student(images) / temperature\n",
        "                    student_log_softmax = F.log_softmax(student_logits, dim=1)\n",
        "                    kld_loss += criterion_kld(student_log_softmax, teacher_softmax)\n",
        "            kld_loss /= len(teachers)\n",
        "\n",
        "            # Compute cross-entropy loss with true labels\n",
        "            student_logits = student(images)\n",
        "            ce_loss = criterion_ce(student_logits, labels)\n",
        "\n",
        "            # Total loss\n",
        "            loss = alpha * kld_loss * (temperature ** 2) + (1 - alpha) * ce_loss\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    return student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uGgcRZLLWlmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf523ef-2c23-4f55-dcf2-d17ccca453a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 86.90%\n",
            "Test Accuracy: 87.48%\n",
            "Test Accuracy: 87.13%\n",
            "Test Accuracy: 87.52%\n",
            "Test Accuracy: 87.15%\n",
            "Test Accuracy: 87.31%\n",
            "Test Accuracy: 87.43%\n",
            "Test Accuracy: 86.71%\n",
            "Test Accuracy: 86.44%\n",
            "Test Accuracy: 86.89%\n",
            "Test Accuracy: 86.75%\n",
            "Test Accuracy: 86.48%\n",
            "Test Accuracy: 86.49%\n",
            "Test Accuracy: 86.28%\n",
            "Test Accuracy: 86.44%\n",
            "Test Accuracy: 86.43%\n",
            "Test Accuracy: 86.05%\n",
            "Test Accuracy: 86.17%\n",
            "Test Accuracy: 85.78%\n",
            "Test Accuracy: 86.04%\n"
          ]
        }
      ],
      "source": [
        "#@title ###*2$^{nd}$ version MTKD,  $\\alpha = 0.5$*\n",
        "\n",
        "num_teachers = [2, 5, 10, 20]\n",
        "\n",
        "if not trained_models:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            student_model_distillated = train_student(teachers[:num], student, 2.0, 0.5, 10)\n",
        "            test_model(student, test_loader)\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "            torch.save(student_model_distillated.state_dict(), f'MTKD_2_student05_{num}_{i}.pth')\n",
        "            with zipfile.ZipFile('MTKD_2_students_05.zip', 'a') as zipf:\n",
        "                zipf.write(f'MTKD_2_student05_{num}_{i}.pth')\n",
        "\n",
        "else:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            with zipfile.ZipFile('MTKD_2_students_05.zip', 'r') as zip_ref:\n",
        "                # Extract the specified file\n",
        "                zip_ref.extract(f'MTKD_2_student05_{num}_{i}.pth')\n",
        "            student_model_distillated.load_state_dict(torch.load(f'MTKD_2_student05_{num}_{i}.pth'))\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "\n",
        "if not trained_models: files.download('MTKD_2_students_05.zip')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###*2$^{nd}$ version MTKD,  $\\alpha = 0.75$*\n",
        "\n",
        "num_teachers = [2, 5, 10, 20]\n",
        "\n",
        "if not trained_models:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            student_model_distillated = train_student(teachers[:num], student, 2.0, 0.75, 10)\n",
        "            test_model(student, test_loader)\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "            torch.save(student_model_distillated.state_dict(), f'MTKD_2_student075_{num}_{i}.pth')\n",
        "            with zipfile.ZipFile('MTKD_2_students_075.zip', 'a') as zipf:\n",
        "                zipf.write(f'MTKD_2_student075_{num}_{i}.pth')\n",
        "\n",
        "else:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            with zipfile.ZipFile('MTKD_2_students_075.zip', 'r') as zip_ref:\n",
        "                # Extract the specified file\n",
        "                zip_ref.extract(f'MTKD_2_student075_{num}_{i}.pth')\n",
        "            student_model_distillated.load_state_dict(torch.load(f'MTKD_2_student075_{num}_{i}.pth'))\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "\n",
        "if not trained_models: files.download('MTKD_2_students_075.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpF-oucHHW7q",
        "outputId": "d243070a-3d78-4609-d029-340d98e989cc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 86.12%\n",
            "Test Accuracy: 85.69%\n",
            "Test Accuracy: 85.81%\n",
            "Test Accuracy: 85.50%\n",
            "Test Accuracy: 85.74%\n",
            "Test Accuracy: 86.06%\n",
            "Test Accuracy: 85.67%\n",
            "Test Accuracy: 85.49%\n",
            "Test Accuracy: 85.49%\n",
            "Test Accuracy: 85.64%\n",
            "Test Accuracy: 85.75%\n",
            "Test Accuracy: 85.72%\n",
            "Test Accuracy: 85.50%\n",
            "Test Accuracy: 85.40%\n",
            "Test Accuracy: 85.60%\n",
            "Test Accuracy: 85.60%\n",
            "Test Accuracy: 85.61%\n",
            "Test Accuracy: 85.25%\n",
            "Test Accuracy: 85.36%\n",
            "Test Accuracy: 85.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del_all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwI4exN1XuLR",
        "outputId": "c225dd5c-d9a4-443a-d8e1-fa260ee4bf8f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: MTKD_2_student05_2_1.pth\n",
            "Deleted: MTKD_2_student05_20_4.pth\n",
            "Deleted: MTKD_2_student05_5_2.pth\n",
            "Deleted: MTKD_2_student075_2_1.pth\n",
            "Deleted: MTKD_2_student075_2_3.pth\n",
            "Deleted: MTKD_2_student075_2_2.pth\n",
            "Deleted: MTKD_2_student075_10_0.pth\n",
            "Deleted: MTKD_student_5_3.pth\n",
            "Deleted: MTKD_student_5_1.pth\n",
            "Deleted: MTKD_student075_5_3.pth\n",
            "Deleted: MTKD_2_student075_20_4.pth\n",
            "Deleted: MTKD_student_20_2.pth\n",
            "Deleted: MTKD_student_20_0.pth\n",
            "Deleted: MTKD_2_student075_5_3.pth\n",
            "Deleted: MTKD_student075_2_3.pth\n",
            "Deleted: MTKD_student075_20_1.pth\n",
            "Deleted: MTKD_student075_5_4.pth\n",
            "Deleted: MTKD_student_5_2.pth\n",
            "Deleted: MTKD_student_2_3.pth\n",
            "Deleted: MTKD_student075_20_3.pth\n",
            "Deleted: MTKD_2_student075_2_4.pth\n",
            "Deleted: MTKD_student_10_2.pth\n",
            "Deleted: MTKD_2_student075_5_2.pth\n",
            "Deleted: MTKD_student075_10_2.pth\n",
            "Deleted: MTKD_student075_2_0.pth\n",
            "Deleted: MTKD_2_student05_2_2.pth\n",
            "Deleted: MTKD_2_student05_20_2.pth\n",
            "Deleted: MTKD_2_student05_5_4.pth\n",
            "Deleted: MTKD_2_student075_10_2.pth\n",
            "Deleted: MTKD_2_student075_20_2.pth\n",
            "Deleted: MTKD_student075_20_2.pth\n",
            "Deleted: MTKD_2_student05_10_0.pth\n",
            "Deleted: MTKD_2_student05_2_0.pth\n",
            "Deleted: MTKD_2_student05_5_3.pth\n",
            "Deleted: MTKD_student075_2_4.pth\n",
            "Deleted: MTKD_2_student075_10_4.pth\n",
            "Deleted: MTKD_student075_10_3.pth\n",
            "Deleted: MTKD_2_student075_5_1.pth\n",
            "Deleted: MTKD_student075_10_4.pth\n",
            "Deleted: MTKD_student075_5_0.pth\n",
            "Deleted: MTKD_student_2_4.pth\n",
            "Deleted: MTKD_student075_10_1.pth\n",
            "Deleted: MTKD_student075_2_2.pth\n",
            "Deleted: MTKD_student_2_2.pth\n",
            "Deleted: MTKD_2_student05_2_4.pth\n",
            "Deleted: MTKD_student_10_4.pth\n",
            "Deleted: MTKD_student_10_3.pth\n",
            "Deleted: MTKD_2_student05_5_0.pth\n",
            "Deleted: MTKD_2_student05_2_3.pth\n",
            "Deleted: MTKD_2_student075_2_0.pth\n",
            "Deleted: MTKD_2_student075_20_0.pth\n",
            "Deleted: MTKD_student_2_1.pth\n",
            "Deleted: MTKD_2_student05_10_2.pth\n",
            "Deleted: MTKD_student_20_1.pth\n",
            "Deleted: MTKD_student_5_0.pth\n",
            "Deleted: MTKD_student075_5_1.pth\n",
            "Deleted: MTKD_student075_2_1.pth\n",
            "Deleted: MTKD_student_5_4.pth\n",
            "Deleted: MTKD_2_student05_10_3.pth\n",
            "Deleted: MTKD_2_student05_20_3.pth\n",
            "Deleted: MTKD_2_student05_5_1.pth\n",
            "Deleted: MTKD_student_20_3.pth\n",
            "Deleted: MTKD_student075_5_2.pth\n",
            "Deleted: MTKD_2_student075_10_1.pth\n",
            "Deleted: MTKD_student075_10_0.pth\n",
            "Deleted: MTKD_2_student05_10_4.pth\n",
            "Deleted: MTKD_student_10_0.pth\n",
            "Deleted: MTKD_2_student075_20_1.pth\n",
            "Deleted: MTKD_2_student075_20_3.pth\n",
            "Deleted: MTKD_2_student05_10_1.pth\n",
            "Deleted: MTKD_2_student075_5_4.pth\n",
            "Deleted: MTKD_2_student05_20_0.pth\n",
            "Deleted: MTKD_student_2_0.pth\n",
            "Deleted: MTKD_student075_20_4.pth\n",
            "Deleted: MTKD_2_student075_10_3.pth\n",
            "Deleted: MTKD_2_student075_5_0.pth\n",
            "Deleted: MTKD_student_20_4.pth\n",
            "Deleted: MTKD_student_10_1.pth\n",
            "Deleted: MTKD_2_student05_20_1.pth\n",
            "Deleted: MTKD_student075_20_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ci--m9XgOEY"
      },
      "source": [
        "### ***Self distillation***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPWithSelfDistillation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLPWithSelfDistillation, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)  # Corrected from self.fc4 to self.fc5 to avoid overwriting\n",
        "\n",
        "        # Classifiers at different depths\n",
        "        self.classifier1 = nn.Linear(512, 10)\n",
        "        self.classifier2 = nn.Linear(256, 10)\n",
        "        self.classifier3 = nn.Linear(128, 10)\n",
        "        self.classifier4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x1 = F.relu(self.fc1(x))\n",
        "        x2 = F.relu(self.fc2(x1))\n",
        "        x3 = F.relu(self.fc3(x2))\n",
        "        x4 = F.relu(self.fc4(x3))\n",
        "        out = self.fc5(x4)  # Corrected from self.fc4(x3) to self.fc5(x4)\n",
        "\n",
        "        c1 = self.classifier1(x1)\n",
        "        c2 = self.classifier2(x2)\n",
        "        c3 = self.classifier3(x3)\n",
        "        c4 = self.classifier4(x4)\n",
        "\n",
        "        return out, c1, c2, c3, c4\n",
        "\n",
        "# Define the loss function for knowledge distillation\n",
        "def loss_fn_kd(outputs, labels, teacher_outputs, alpha=0.5, T=2):\n",
        "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs / T, dim=1),\n",
        "                             F.softmax(teacher_outputs / T, dim=1)) * (alpha * T * T) + \\\n",
        "              F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "    return KD_loss\n",
        "\n",
        "# Training function for a single model\n",
        "def train_self(model, alpha=0.5, epochs=10):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output, c1, c2, c3, c4 = model(data)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss1 = F.cross_entropy(c1, target)\n",
        "            loss2 = F.cross_entropy(c2, target)\n",
        "            loss3 = F.cross_entropy(c3, target)\n",
        "            loss4 = F.cross_entropy(c4, target)\n",
        "            loss_final = F.cross_entropy(output, target)\n",
        "\n",
        "            # Self-distillation losses\n",
        "            kl_loss1 = loss_fn_kd(c1, target, output, alpha)\n",
        "            kl_loss2 = loss_fn_kd(c2, target, output, alpha)\n",
        "            kl_loss3 = loss_fn_kd(c3, target, output, alpha)\n",
        "            kl_loss4 = loss_fn_kd(c4, target, output, alpha)\n",
        "\n",
        "            # Total loss\n",
        "            loss = loss1 + loss2 + loss3 + loss4 + loss_final + kl_loss1 + kl_loss2 + kl_loss3 + kl_loss4\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss / len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mWOZRrhMY3b2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testSTKD(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output, _, _, _, _ = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "5xox2vUcnycZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9YwcTKdAM8dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c94083a-4f7e-446c-aa37-c1520e6008a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 88.68%\n",
            "Accuracy: 88.45%\n",
            "Accuracy: 88.64%\n",
            "Accuracy: 88.44%\n",
            "Accuracy: 88.61%\n",
            "Accuracy: 87.99%\n",
            "Accuracy: 88.22%\n",
            "Accuracy: 88.23%\n",
            "Accuracy: 88.38%\n",
            "Accuracy: 88.12%\n",
            "Accuracy: 88.96%\n",
            "Accuracy: 88.53%\n",
            "Accuracy: 88.77%\n",
            "Accuracy: 87.76%\n",
            "Accuracy: 88.29%\n",
            "Accuracy: 88.51%\n",
            "Accuracy: 88.21%\n",
            "Accuracy: 88.41%\n",
            "Accuracy: 88.13%\n",
            "Accuracy: 88.30%\n"
          ]
        }
      ],
      "source": [
        "teachers_STKD_05 = []\n",
        "num_teachers = 20\n",
        "if not trained_models:\n",
        "    for i in range(num_teachers):\n",
        "        torch.manual_seed(i)\n",
        "        teacher = MLPWithSelfDistillation().to(device)\n",
        "        teacher = train_self(teacher)\n",
        "        teacher_acc = testSTKD(teacher, test_loader)\n",
        "        teachers_STKD_05.append(teacher)\n",
        "        torch.save(teacher.state_dict(), f'teacher_STKD_{i}.pth')\n",
        "        with zipfile.ZipFile(f'STKD_05.zip', 'a') as zipf:\n",
        "            zipf.write(f'teacher_STKD_{i}.pth')\n",
        "    files.download(f'STKD_05.zip')\n",
        "else:\n",
        "    for i in range(num_teachers):\n",
        "        torch.manual_seed(i)\n",
        "        teacher = MLPWithSelfDistillation().to(device)\n",
        "        with zipfile.ZipFile('STKD_05.zip', 'r') as zip_ref:\n",
        "            # Extract the specified file\n",
        "            zip_ref.extract(f'teacher_STKD_{i}.pth')\n",
        "        teacher.load_state_dict(torch.load(f'teacher_STKD_{i}.pth'))\n",
        "        teacher_accuracy = testSTKD(teacher, test_loader)\n",
        "        teachers_STKD_05.append(teacher)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teachers_STKD_025 = []\n",
        "num_teachers = 20\n",
        "if not trained_models:\n",
        "    for i in range(num_teachers):\n",
        "        torch.manual_seed(i)\n",
        "        teacher = MLPWithSelfDistillation().to(device)\n",
        "        teacher = train_self(teacher)\n",
        "        teacher_acc = testSTKD(teacher, test_loader)\n",
        "        teachers_STKD_025.append(teacher)\n",
        "        torch.save(teacher.state_dict(), f'teacher_STKD_025_{i}.pth')\n",
        "        with zipfile.ZipFile(f'STKD_025.zip', 'a') as zipf:\n",
        "            zipf.write(f'teacher_STKD_025_{i}.pth')\n",
        "    files.download(f'STKD_025.zip')\n",
        "else:\n",
        "    for i in range(num_teachers):\n",
        "        torch.manual_seed(i)\n",
        "        teacher = MLPWithSelfDistillation().to(device)\n",
        "        with zipfile.ZipFile('STKD_025.zip', 'r') as zip_ref:\n",
        "            # Extract the specified file\n",
        "            zip_ref.extract(f'teacher_STKD_025_{i}.pth')\n",
        "        teacher.load_state_dict(torch.load(f'teacher_STKD_025_{i}.pth'))\n",
        "        teacher_accuracy = testSTKD(teacher, test_loader)\n",
        "        teachers_STKD_025.append(teacher)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzdjrvSuh69M",
        "outputId": "23f91b7d-7cca-46ce-855f-8d2c43f0b406"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 88.68%\n",
            "Accuracy: 88.45%\n",
            "Accuracy: 88.64%\n",
            "Accuracy: 88.44%\n",
            "Accuracy: 88.61%\n",
            "Accuracy: 87.99%\n",
            "Accuracy: 88.22%\n",
            "Accuracy: 88.23%\n",
            "Accuracy: 88.38%\n",
            "Accuracy: 88.12%\n",
            "Accuracy: 88.96%\n",
            "Accuracy: 88.53%\n",
            "Accuracy: 88.77%\n",
            "Accuracy: 87.76%\n",
            "Accuracy: 88.29%\n",
            "Accuracy: 88.51%\n",
            "Accuracy: 88.21%\n",
            "Accuracy: 88.41%\n",
            "Accuracy: 88.13%\n",
            "Accuracy: 88.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del_all()"
      ],
      "metadata": {
        "id": "7Hq77S49Ng0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ffe4b6-131e-4dfc-89a9-d7c09be416aa"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: teacher_STKD_025_0.pth\n",
            "Deleted: teacher_STKD_4.pth\n",
            "Deleted: teacher_STKD_025_9.pth\n",
            "Deleted: teacher_STKD_7.pth\n",
            "Deleted: teacher_STKD_025_17.pth\n",
            "Deleted: teacher_STKD_025_11.pth\n",
            "Deleted: teacher_STKD_025_4.pth\n",
            "Deleted: teacher_STKD_025_12.pth\n",
            "Deleted: teacher_STKD_15.pth\n",
            "Deleted: teacher_STKD_18.pth\n",
            "Deleted: teacher_STKD_025_6.pth\n",
            "Deleted: teacher_STKD_0.pth\n",
            "Deleted: teacher_STKD_6.pth\n",
            "Deleted: teacher_STKD_5.pth\n",
            "Deleted: teacher_STKD_025_7.pth\n",
            "Deleted: teacher_STKD_025_18.pth\n",
            "Deleted: teacher_STKD_025_15.pth\n",
            "Deleted: teacher_STKD_16.pth\n",
            "Deleted: teacher_STKD_10.pth\n",
            "Deleted: teacher_STKD_17.pth\n",
            "Deleted: teacher_STKD_025_3.pth\n",
            "Deleted: teacher_STKD_025_14.pth\n",
            "Deleted: teacher_STKD_11.pth\n",
            "Deleted: teacher_STKD_8.pth\n",
            "Deleted: teacher_STKD_12.pth\n",
            "Deleted: teacher_STKD_025_13.pth\n",
            "Deleted: teacher_STKD_13.pth\n",
            "Deleted: teacher_STKD_19.pth\n",
            "Deleted: teacher_STKD_025_1.pth\n",
            "Deleted: teacher_STKD_025_8.pth\n",
            "Deleted: teacher_STKD_025_16.pth\n",
            "Deleted: teacher_STKD_025_19.pth\n",
            "Deleted: teacher_STKD_025_5.pth\n",
            "Deleted: teacher_STKD_2.pth\n",
            "Deleted: teacher_STKD_025_2.pth\n",
            "Deleted: teacher_STKD_9.pth\n",
            "Deleted: teacher_STKD_14.pth\n",
            "Deleted: teacher_STKD_3.pth\n",
            "Deleted: teacher_STKD_025_10.pth\n",
            "Deleted: teacher_STKD_1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0NpyiuIfM4VG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48474eef-53f2-46e2-eb97-d5cad60f5b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 88.17%\n",
            "Accuracy: 89.12%\n",
            "Accuracy: 88.40%\n",
            "Accuracy: 88.84%\n",
            "Accuracy: 88.42%\n",
            "Accuracy: 88.04%\n",
            "Accuracy: 88.64%\n",
            "Accuracy: 88.30%\n",
            "Accuracy: 88.43%\n",
            "Accuracy: 88.75%\n",
            "Accuracy: 88.87%\n",
            "Accuracy: 88.54%\n",
            "Accuracy: 88.66%\n",
            "Accuracy: 88.97%\n",
            "Accuracy: 87.88%\n",
            "Accuracy: 88.34%\n",
            "Accuracy: 88.35%\n",
            "Accuracy: 88.60%\n",
            "Accuracy: 88.45%\n",
            "Accuracy: 88.89%\n"
          ]
        }
      ],
      "source": [
        "teachers_STKD_075 = []\n",
        "num_teachers = 20\n",
        "if not trained_models:\n",
        "    for i in range(num_teachers):\n",
        "        torch.manual_seed(i)\n",
        "        teacher = MLPWithSelfDistillation().to(device)\n",
        "        teacher = train_self(teacher, alpha = 0.75)\n",
        "        teachers_STKD_075.append(teachers)\n",
        "        torch.save(teacher.state_dict(), f'teacher_{i}.pth')\n",
        "        with zipfile.ZipFile(f'STKD_075.zip', 'a') as zipf:\n",
        "            zipf.write(f'teacher_{i}.pth')\n",
        "    files.download(f'STKD_075.zip')\n",
        "else:\n",
        "    for i in range(num_teachers):\n",
        "        torch.manual_seed(i)\n",
        "        teacher = MLPWithSelfDistillation().to(device)\n",
        "        with zipfile.ZipFile('STKD_075.zip', 'r') as zip_ref:\n",
        "            # Extract the specified file\n",
        "            zip_ref.extract(f'teacher_{i}.pth')\n",
        "        teacher.load_state_dict(torch.load(f'teacher_{i}.pth'))\n",
        "        teacher_accuracy = testSTKD(teacher, test_loader)\n",
        "        teachers_STKD_075.append(teacher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Uew6GpPYhoei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925d631b-c2e3-44e4-a277-7ef07b96f1e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: teacher_7.pth\n",
            "Deleted: teacher_19.pth\n",
            "Deleted: teacher_18.pth\n",
            "Deleted: teacher_0.pth\n",
            "Deleted: teacher_11.pth\n",
            "Deleted: teacher_14.pth\n",
            "Deleted: teacher_12.pth\n",
            "Deleted: teacher_4.pth\n",
            "Deleted: teacher_17.pth\n",
            "Deleted: teacher_3.pth\n",
            "Deleted: teacher_16.pth\n",
            "Deleted: teacher_6.pth\n",
            "Deleted: teacher_1.pth\n",
            "Deleted: teacher_8.pth\n",
            "Deleted: teacher_10.pth\n",
            "Deleted: teacher_15.pth\n",
            "Deleted: teacher_5.pth\n",
            "Deleted: teacher_9.pth\n",
            "Deleted: teacher_13.pth\n",
            "Deleted: teacher_2.pth\n"
          ]
        }
      ],
      "source": [
        "del_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JirIWJU0-CIX"
      },
      "source": [
        "### ***Unisco i concetti***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_student_model_multi_teacher(teachers, student, epochs = 10, alpha=0.5, T=2.0, device='cpu'):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "    for teacher in teachers:\n",
        "        teacher.eval()  # Set all teacher models to evaluation mode\n",
        "    student.train()  # Student in training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher models - do not save gradients\n",
        "            teacher_logits = []\n",
        "            with torch.no_grad():\n",
        "                for teacher in teachers:\n",
        "                    t_logits,_,_,_,_ = teacher(inputs)\n",
        "                    teacher_logits.append(t_logits)\n",
        "\n",
        "            # Average the logits from the teacher models\n",
        "            avg_teacher_logits = sum(teacher_logits) / len(teacher_logits)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            # Soften the teacher and student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(avg_teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = nn.KLDivLoss()(soft_prob, soft_targets) * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = alpha * soft_targets_loss + (1 - alpha) * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "RZ3QbB-WHMZW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "sGTGNkAj-3B0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a19793-c1ff-456f-e85e-d3a9b1297e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 87.07%\n",
            "Test Accuracy: 86.82%\n",
            "Test Accuracy: 87.17%\n",
            "Test Accuracy: 86.01%\n",
            "Test Accuracy: 85.74%\n",
            "Test Accuracy: 87.09%\n",
            "Test Accuracy: 86.86%\n",
            "Test Accuracy: 86.91%\n",
            "Test Accuracy: 86.09%\n",
            "Test Accuracy: 85.51%\n",
            "Test Accuracy: 87.25%\n",
            "Test Accuracy: 86.76%\n",
            "Test Accuracy: 86.94%\n",
            "Test Accuracy: 86.33%\n",
            "Test Accuracy: 85.75%\n",
            "Test Accuracy: 87.05%\n",
            "Test Accuracy: 86.90%\n",
            "Test Accuracy: 86.80%\n",
            "Test Accuracy: 86.21%\n",
            "Test Accuracy: 85.42%\n"
          ]
        }
      ],
      "source": [
        "trained_models = True\n",
        "\n",
        "num_teachers = [2,5,10,20]\n",
        "\n",
        "num_iterations = 5\n",
        "\n",
        "if not trained_models:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            student_model_distillated = train_student_model_multi_teacher(teachers[:num], student_model_distillated)\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "            torch.save(student_model_distillated.state_dict(), f'Final_2_student05_{num}_{i}.pth')\n",
        "            with zipfile.ZipFile('Final_2_students_05.zip', 'a') as zipf:\n",
        "                zipf.write(f'Final_2_student05_{num}_{i}.pth')\n",
        "    files.download('Final_2_students_05.zip')\n",
        "else:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            with zipfile.ZipFile('Final_2_students_05.zip', 'r') as zip_ref:\n",
        "                # Extract the specified file\n",
        "                zip_ref.extract(f'Final_2_student05_{num}_{i}.pth')\n",
        "            student_model_distillated.load_state_dict(torch.load(f'Final_2_student05_{num}_{i}.pth'))\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del_all()"
      ],
      "metadata": {
        "id": "TS6oI-AYZduK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a177cd6e-19e0-42ff-bb87-6adacc3c813b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: Final_2_student05_5_0.pth\n",
            "Deleted: Final_2_student05_10_2.pth\n",
            "Deleted: Final_2_student05_2_4.pth\n",
            "Deleted: Final_2_student05_20_0.pth\n",
            "Deleted: Final_2_student05_10_0.pth\n",
            "Deleted: Final_2_student05_2_1.pth\n",
            "Deleted: Final_2_student05_5_4.pth\n",
            "Deleted: Final_2_student05_10_1.pth\n",
            "Deleted: Final_2_student05_20_2.pth\n",
            "Deleted: Final_2_student05_10_3.pth\n",
            "Deleted: Final_2_student05_5_3.pth\n",
            "Deleted: Final_2_student05_5_1.pth\n",
            "Deleted: Final_2_student05_20_4.pth\n",
            "Deleted: Final_2_student05_20_1.pth\n",
            "Deleted: Final_2_student05_2_0.pth\n",
            "Deleted: Final_2_student05_20_3.pth\n",
            "Deleted: Final_2_student05_2_3.pth\n",
            "Deleted: Final_2_student05_5_2.pth\n",
            "Deleted: Final_2_student05_2_2.pth\n",
            "Deleted: Final_2_student05_10_4.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MTKD versione 2\n",
        "\n",
        "def train_student_MTKD_2(teachers, student, temperature = 2.0, alpha = 0.5, epochs = 10):\n",
        "    dataloader = train_loader\n",
        "    optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "    criterion_ce = nn.CrossEntropyLoss()\n",
        "    criterion_kld = nn.KLDivLoss(reduction='batchmean')\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Get teacher logits and compute KLD loss\n",
        "            kld_loss = 0\n",
        "            for teacher in teachers:\n",
        "                teacher.eval()\n",
        "                with torch.no_grad():\n",
        "                    T_L, _, _, _, _ = teacher(images)\n",
        "                    teacher_logits = T_L / temperature\n",
        "                    teacher_softmax = F.softmax(teacher_logits, dim=1)\n",
        "                    student_logits = student(images) / temperature\n",
        "                    student_log_softmax = F.log_softmax(student_logits, dim=1)\n",
        "                    kld_loss += criterion_kld(student_log_softmax, teacher_softmax)\n",
        "            kld_loss /= len(teachers)\n",
        "\n",
        "            # Compute cross-entropy loss with true labels\n",
        "            student_logits = student(images)\n",
        "            ce_loss = criterion_ce(student_logits, labels)\n",
        "\n",
        "            # Total loss\n",
        "            loss = alpha * kld_loss * (temperature ** 2) + (1 - alpha) * ce_loss\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    return student"
      ],
      "metadata": {
        "id": "EIkzZb2yZlhU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_teachers = [10, 20]\n",
        "\n",
        "trained_models = False\n",
        "\n",
        "num_iterations = 5\n",
        "\n",
        "if not trained_models:\n",
        "    for num in num_teachers:\n",
        "        if num == 10: j = 2\n",
        "        else: j = 0\n",
        "        for i in range(j, num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            student_model_distillated = train_student_MTKD_2(teachers_STKD_05[:num], student_model_distillated)\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "            torch.save(student_model_distillated.state_dict(), f'Final_3_student05_{num}_{i}.pth')\n",
        "            with zipfile.ZipFile('Final_3_students_05.zip', 'a') as zipf:\n",
        "                zipf.write(f'Final_3_student05_{num}_{i}.pth')\n",
        "    files.download('Final_3_students_05.zip')\n",
        "else:\n",
        "    for num in num_teachers:\n",
        "        for i in range(num_iterations):\n",
        "            torch.manual_seed(i)\n",
        "            student_model_distillated = StudentModel().to(device)\n",
        "            with zipfile.ZipFile('Final_3_students_05.zip', 'r') as zip_ref:\n",
        "                # Extract the specified file\n",
        "                zip_ref.extract(f'Final_3_student05_{num}_{i}.pth')\n",
        "            student_model_distillated.load_state_dict(torch.load(f'Final_3_student05_{num}_{i}.pth'))\n",
        "            student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n"
      ],
      "metadata": {
        "id": "pBW8h4vuUayL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "5db95c2b-5819-4c54-866c-47ca54588b04"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.0359\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-a7029dfc2823>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mstudent_model_distillated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStudentModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mstudent_model_distillated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_student_MTKD_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteachers_STKD_05\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_model_distillated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mstudent_distillated_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model_distillated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model_distillated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Final_3_student05_{num}_{i}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-698d5f7381ff>\u001b[0m in \u001b[0;36mtrain_student_MTKD_2\u001b[0;34m(teachers, student, temperature, alpha, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Test Accuracy: 86.90%\n",
        "Test Accuracy: 87.48%\n",
        "Test Accuracy: 87.13%\n",
        "Test Accuracy: 87.52%\n",
        "Test Accuracy: 87.15%\n",
        "Test Accuracy: 87.31%\n",
        "Test Accuracy: 87.43%\n",
        "Test Accuracy: 86.71%\n",
        "Test Accuracy: 86.44%\n",
        "Test Accuracy: 86.89%\n",
        "Test Accuracy: 86.75%\n",
        "Test Accuracy: 86.48%\n",
        "Test Accuracy: 86.49%\n",
        "Test Accuracy: 86.28%\n",
        "Test Accuracy: 86.44%\n",
        "Test Accuracy: 86.43%\n",
        "Test Accuracy: 86.05%\n",
        "Test Accuracy: 86.17%\n",
        "Test Accuracy: 85.78%\n",
        "Test Accuracy: 86.04%\n",
        "\n",
        "forse con troppi insegnanti non regolarizzo bene e overfitto un po'\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "jcYtYpk1oddW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset, DataLoader, random_split\n",
        "\n",
        "# Define the Teacher Model\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Function to train the teacher models\n",
        "def train_teacher(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "'''\n",
        "# Load the FashionMNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Number of teachers\n",
        "num_teachers = 20\n",
        "\n",
        "# Function to create random splits\n",
        "def create_random_splits(dataset, num_teachers):\n",
        "    split_size = len(dataset) // 5  # Half of the dataset for each teacher\n",
        "    subsets = []\n",
        "\n",
        "    for _ in range(num_teachers):\n",
        "        subset1, _ = random_split(dataset, [split_size, len(dataset) - split_size])\n",
        "        subsets.append(subset1)\n",
        "\n",
        "    return subsets\n",
        "\n",
        "# Create random half splits\n",
        "subsets = create_random_splits(train_dataset, num_teachers)\n",
        "\n",
        "# Initialize and train each teacher on their respective random half subset\n",
        "teacher_models = []\n",
        "for i in range(num_teachers):\n",
        "    teacher_model = TeacherModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
        "\n",
        "    train_loader = DataLoader(subsets[i], batch_size=64, shuffle=True)\n",
        "\n",
        "    print(f\"Training Teacher {i+1}/{num_teachers}\")\n",
        "    train_teacher(teacher_model, train_loader, criterion, optimizer, epochs=10)\n",
        "    test_model(teacher_model, test_loader)\n",
        "    teacher_models.append(teacher_model)\n",
        "\n",
        "print(\"Training of all teacher models is complete.\")\n",
        "'''\n"
      ],
      "metadata": {
        "id": "BqrekuUF2dvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "e5729d0a-a7d0-4871-ea5a-62b3d14ba2b8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Teacher 1/20\n",
            "Epoch 1, Loss: 0.8648814990165385\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-d2c81e1e76dc>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Teacher {i+1}/{num_teachers}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mtrain_teacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mteacher_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-d2c81e1e76dc>\u001b[0m in \u001b[0;36mtrain_teacher\u001b[0;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3069\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3072\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "qui sotto posso riscrivere il codice che dava il 40 di accuracy con teacher MLP allenati su parti diverse del dataset"
      ],
      "metadata": {
        "id": "J4qWDpxI8m-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def combine_teacher_predictions(teacher_models, test_loader, temperature=2.0):\n",
        "    \"\"\"\n",
        "    Combine predictions from multiple teacher models on the entire test set and calculate accuracy.\n",
        "\n",
        "    Args:\n",
        "        teacher_models (list of nn.Module): List of teacher models.\n",
        "        test_loader (DataLoader): DataLoader for the test set.\n",
        "        method (str): Method to combine predictions ('avg', 'max', 'vote').\n",
        "        temperature (float): Temperature for scaling logits before combining predictions.\n",
        "\n",
        "    Returns:\n",
        "        accuracy (float): Accuracy of the combined predictions on the test set.\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Collect logits from all teacher models\n",
        "        logits_list = []\n",
        "        for teacher in teacher_models:\n",
        "            teacher.eval()\n",
        "            with torch.no_grad():\n",
        "                #logits, _, _, _, _ = teacher(inputs)\n",
        "                logits = teacher(inputs)\n",
        "\n",
        "                # Apply temperature scaling\n",
        "                logits = logits / temperature\n",
        "                logits_list.append(logits)\n",
        "\n",
        "        # Stack logits from all teachers: shape [num_teachers, batch_size, num_classes]\n",
        "        stacked_logits = torch.stack(logits_list)\n",
        "\n",
        "        if method == 'avg':\n",
        "            # Average the logits across all teachers\n",
        "            combined_logits = torch.mean(stacked_logits, dim=0)\n",
        "\n",
        "            raise ValueError(\"Method must be 'avg', 'max', or 'vote'\")\n",
        "\n",
        "        # Final prediction based on the combined logits\n",
        "        final_predictions = torch.argmax(combined_logits, dim=1)\n",
        "\n",
        "        # Update correct predictions count and total samples\n",
        "        correct_predictions += (final_predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return accuracy\n",
        "'''\n",
        "num_teachers = [2,5,10,20]\n",
        "for t in num_teachers:\n",
        "    print(f\"Test Accuracy: {combine_teacher_predictions(teachers_STKD_05[:t], test_loader, method='avg')}\")\n",
        "    print(f\"Test Accuracy: {combine_teacher_predictions(teachers_STKD_025[:t], test_loader, method='avg')}\")\n",
        "    print(f\"Test Accuracy: {combine_teacher_predictions(teachers_STKD_075[:t], test_loader, method='avg')}\")\n",
        "'''\n",
        "# Example usage:\n",
        "# Assuming you have 20 trained teacher models in a list called `teacher_models`\n",
        "# and some input data `input_data` (e.g., a batch of images)\n",
        "\n",
        "# teacher_models = [teacher1, teacher2, ..., teacher20]  # Your 20 teacher models\n",
        "# input_data = ...  # Your input data\n",
        "\n",
        "# Combine predictions and get final predictions\n",
        "# combined_logits, final_predictions = combine_teacher_predictions(teacher_models, input_data, method='avg')\n"
      ],
      "metadata": {
        "id": "iqiR4j9JEyk2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d45eeb19-70ff-455f-ef4c-7ab1ecd4d44f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnum_teachers = [2,5,10,20]\\nfor t in num_teachers:\\n    print(f\"Test Accuracy: {combine_teacher_predictions(teachers_STKD_05[:t], test_loader, method=\\'avg\\')}\")\\n    print(f\"Test Accuracy: {combine_teacher_predictions(teachers_STKD_025[:t], test_loader, method=\\'avg\\')}\")\\n    print(f\"Test Accuracy: {combine_teacher_predictions(teachers_STKD_075[:t], test_loader, method=\\'avg\\')}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "2. Over-regularization\n",
        "Smoothened Outputs: With more teachers, especially when using a high temperature in the softmax function, the output distributions become more smooth and less distinct. This smoothening can lead to over-regularization, where the student model's learning is overly constrained, resulting in a loss of sharpness and specificity in predictions.\n",
        "Reduced Model Capacity Utilization: The student model may not fully utilize its capacity when learning from overly smooth or averaged teacher outputs, leading to underfitting.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "IZRdft7nFjov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3192367a-0218-4bea-a18f-41cbc2c3cc98"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n2. Over-regularization\\nSmoothened Outputs: With more teachers, especially when using a high temperature in the softmax function, the output distributions become more smooth and less distinct. This smoothening can lead to over-regularization, where the student model's learning is overly constrained, resulting in a loss of sharpness and specificity in predictions.\\nReduced Model Capacity Utilization: The student model may not fully utilize its capacity when learning from overly smooth or averaged teacher outputs, leading to underfitting.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del_all()"
      ],
      "metadata": {
        "id": "VBUtk3tfTtWF"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train function\n",
        "def train(model, epochs, learning_rate=0.001):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')"
      ],
      "metadata": {
        "id": "eQc2vQnZTefF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Insegnanti diversi***"
      ],
      "metadata": {
        "id": "VVzVd7wmHxj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Jp6bTqUGvwjm"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define CNN Teacher Model\n",
        "class CNNTeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNTeacherModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ShallowDenseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShallowDenseNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.denseblock = nn.Sequential(\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.fc = nn.Linear(8 * 28 * 28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.denseblock(x) + x  # Skip connection\n",
        "        x = x.view(-1, 8 * 28 * 28)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FireModule(nn.Module):\n",
        "    def __init__(self, in_channels, squeeze_channels, expand_channels):\n",
        "        super(FireModule, self).__init__()\n",
        "        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=1)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_channels, expand_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            torch.relu(self.expand1x1(x)),\n",
        "            torch.relu(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "class SimpleSqueezeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleSqueezeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2)\n",
        "        self.fire1 = FireModule(16, 16, 32)\n",
        "        self.fire2 = FireModule(64, 16, 32)\n",
        "        self.conv2 = nn.Conv2d(64, 10, kernel_size=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.fire1(x)\n",
        "        x = self.fire2(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3EP7gwHwB3B3"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_models = True\n",
        "\n",
        "num_teachers = [2,5,10,20]\n",
        "final_teachers = []\n",
        "\n",
        "num_iterations = 5\n",
        "\n",
        "if not trained_models:\n",
        "    for i in range(num_iterations):\n",
        "        torch.manual_seed(i)\n",
        "        cnn = CNNTeacherModel()\n",
        "        train(cnn, 10)\n",
        "        torch.save(cnn.state_dict(), f'CNN_teacher_{i}.pth')\n",
        "        with zipfile.ZipFile('Last_Teachers.zip', 'a') as zipf: zipf.write(f'CNN_teacher_{i}.pth')\n",
        "        torch.manual_seed(i)\n",
        "        shallowdensenet = ShallowDenseNet()\n",
        "        train(shallowdensenet, 10)\n",
        "        torch.save(shallowdensenet.state_dict(), f'shallowdensenet_teacher_{i}.pth')\n",
        "        with zipfile.ZipFile('Last_Teachers.zip', 'a') as zipf: zipf.write(f'shallowdensenet_teacher_{i}.pth')\n",
        "        torch.manual_seed(i)\n",
        "        simple_squeeze_net = SimpleSqueezeNet()\n",
        "        train(simple_squeeze_net, 10)\n",
        "        torch.save(simple_squeeze_net.state_dict(), f'simple_squeeze_net_teacher_{i}.pth')\n",
        "        with zipfile.ZipFile('Last_Teachers.zip', 'a') as zipf: zipf.write(f'simple_squeeze_net_teacher_{i}.pth')\n",
        "\n",
        "    files.download('Last_Teachers.zip')\n",
        "else:\n",
        "    for i in range(num_iterations):\n",
        "        torch.manual_seed(i)\n",
        "        with zipfile.ZipFile('Last_Teachers.zip', 'r') as zip_ref:\n",
        "            zip_ref.extract(f'CNN_teacher_{i}.pth')\n",
        "        cnn = CNNTeacherModel().to(device)\n",
        "        cnn.load_state_dict(torch.load(f'CNN_teacher_{i}.pth'))\n",
        "        cnn_acc = test_model(cnn, test_loader)\n",
        "\n",
        "        with zipfile.ZipFile('Last_Teachers.zip', 'r') as zip_ref:\n",
        "            zip_ref.extract(f'shallowdensenet_teacher_{i}.pth')\n",
        "        sdnt = ShallowDenseNet().to(device)\n",
        "        sdnt.load_state_dict(torch.load(f'shallowdensenet_teacher_{i}.pth'))\n",
        "        sdnt_acc = test_model(sdnt, test_loader)\n",
        "\n",
        "        '''\n",
        "        with zipfile.ZipFile('Last_Teachers.zip', 'r') as zip_ref:\n",
        "            zip_ref.extract(f'simple_squeeze_net_teacher_{i}.pth')\n",
        "        SSN = SimpleSqueezeNet().to(device)\n",
        "        SSN.load_state_dict(torch.load(f'simple_squeeze_net_teacher_{i}.pth'))\n",
        "        SSN_acc = test_model(SSN, test_loader)\n",
        "        '''\n",
        "        with zipfile.ZipFile('MTKD.zip', 'r') as zip_ref:\n",
        "            zip_ref.extract(f'teacher_{i}.pth')\n",
        "        SSN = TeacherModel().to(device)\n",
        "        SSN.load_state_dict(torch.load(f'teacher_{i}.pth'))\n",
        "        SSN_acc = test_model(SSN, test_loader)\n",
        "\n",
        "        final_teachers.append(cnn)\n",
        "        final_teachers.append(sdnt)\n",
        "        final_teachers.append(SSN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k26iHRwe5IDP",
        "outputId": "9359ba7b-bcfb-4428-9330-09d2e4030499"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 91.63%\n",
            "Test Accuracy: 89.44%\n",
            "Test Accuracy: 88.27%\n",
            "Test Accuracy: 92.02%\n",
            "Test Accuracy: 89.14%\n",
            "Test Accuracy: 88.23%\n",
            "Test Accuracy: 92.02%\n",
            "Test Accuracy: 88.89%\n",
            "Test Accuracy: 87.95%\n",
            "Test Accuracy: 91.93%\n",
            "Test Accuracy: 89.51%\n",
            "Test Accuracy: 88.32%\n",
            "Test Accuracy: 91.87%\n",
            "Test Accuracy: 89.32%\n",
            "Test Accuracy: 88.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combine_teacher_predictions(final_teachers, test_loader, method='avg', temperature=2.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDAXpKF2FgZ3",
        "outputId": "0220c4cd-98da-43ca-80ae-deb0ea233294"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9303"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_teachers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQKBBvT2QT6b",
        "outputId": "ee76ebb5-f51e-4b8b-e6b0-f829abc670ab"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CNNTeacherModel(\n",
              "   (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "   (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "   (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              " ),\n",
              " ShallowDenseNet(\n",
              "   (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (denseblock): Sequential(\n",
              "     (0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (1): ReLU(inplace=True)\n",
              "     (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (4): ReLU(inplace=True)\n",
              "     (5): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   )\n",
              "   (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
              " ),\n",
              " TeacherModel(\n",
              "   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
              "   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "   (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "   (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "   (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
              " ),\n",
              " CNNTeacherModel(\n",
              "   (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "   (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "   (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              " ),\n",
              " ShallowDenseNet(\n",
              "   (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (denseblock): Sequential(\n",
              "     (0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (1): ReLU(inplace=True)\n",
              "     (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (4): ReLU(inplace=True)\n",
              "     (5): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   )\n",
              "   (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
              " ),\n",
              " TeacherModel(\n",
              "   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
              "   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "   (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "   (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "   (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
              " ),\n",
              " CNNTeacherModel(\n",
              "   (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "   (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "   (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              " ),\n",
              " ShallowDenseNet(\n",
              "   (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (denseblock): Sequential(\n",
              "     (0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (1): ReLU(inplace=True)\n",
              "     (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (4): ReLU(inplace=True)\n",
              "     (5): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   )\n",
              "   (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
              " ),\n",
              " TeacherModel(\n",
              "   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
              "   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "   (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "   (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "   (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
              " ),\n",
              " CNNTeacherModel(\n",
              "   (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "   (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "   (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              " ),\n",
              " ShallowDenseNet(\n",
              "   (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (denseblock): Sequential(\n",
              "     (0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (1): ReLU(inplace=True)\n",
              "     (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (4): ReLU(inplace=True)\n",
              "     (5): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   )\n",
              "   (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
              " ),\n",
              " TeacherModel(\n",
              "   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
              "   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "   (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "   (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "   (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
              " ),\n",
              " CNNTeacherModel(\n",
              "   (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "   (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "   (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              " ),\n",
              " ShallowDenseNet(\n",
              "   (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (denseblock): Sequential(\n",
              "     (0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (1): ReLU(inplace=True)\n",
              "     (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "     (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (4): ReLU(inplace=True)\n",
              "     (5): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   )\n",
              "   (fc): Linear(in_features=6272, out_features=10, bias=True)\n",
              " ),\n",
              " TeacherModel(\n",
              "   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
              "   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "   (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "   (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "   (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###*2$^{nd}$ version MTKD,  $\\alpha = 0.5$*\n",
        "\n",
        "num_teachers = [2, 5, 10, 20]\n",
        "\n",
        "trained_models = False\n",
        "\n",
        "if not trained_models:\n",
        "    for i in range(num_iterations):\n",
        "        torch.manual_seed(i)\n",
        "        student_model_distillated = StudentModel().to(device)\n",
        "        student_model_distillated = train_student_model_multi_teacher(final_teachers, student_model_distillated, train_loader)\n",
        "        test_model(student_model_distillated, test_loader)\n",
        "        student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "        torch.save(student_model_distillated.state_dict(), f'MTKD_finale_student05_{i}.pth')\n",
        "        with zipfile.ZipFile('MTKD_finale.zip', 'a') as zipf:\n",
        "            zipf.write(f'MTKD_finale_student05_{i}.pth')\n",
        "\n",
        "else:\n",
        "    for i in range(num_iterations):\n",
        "        torch.manual_seed(i)\n",
        "        student_model_distillated = StudentModel().to(device)\n",
        "        with zipfile.ZipFile('MTKD_finale.zip', 'r') as zip_ref:\n",
        "            # Extract the specified file\n",
        "            zip_ref.extract(f'MTKD_finale_student05_{i}.pth')\n",
        "        student_model_distillated.load_state_dict(torch.load(f'MTKD_finale_student05_{i}.pth'))\n",
        "        student_distillated_accuracy = test_model(student_model_distillated, test_loader)\n",
        "\n",
        "if not trained_models: files.download('MTKD_finale.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQrDfj50LPk6",
        "outputId": "2a121612-71a7-4b3a-bd88-68f4eea64c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2976: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3645\n",
            "Epoch 2/10, Loss: 0.2633\n",
            "Epoch 3/10, Loss: 0.2413\n",
            "Epoch 4/10, Loss: 0.2250\n",
            "Epoch 5/10, Loss: 0.2139\n",
            "Epoch 6/10, Loss: 0.2047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combine_teacher_predictions(final_teachers, test_loader, temperature=1.0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZX3D7nPR8yF",
        "outputId": "9b752070-7dbe-4d7f-ca4b-8f01ca664dc5"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10000, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input, label in test_loader:\n",
        "  print(student(input).shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpSghb8vTLnS",
        "outputId": "8d56f20a-d2dc-4d4d-b010-02a1af20019f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def combine_teacher_predictions(teacher_models, test_loader, temperature=2.0):\n",
        "    \"\"\"\n",
        "    Combine predictions from multiple teacher models on the entire test set and calculate accuracy.\n",
        "\n",
        "    Args:\n",
        "        teacher_models (list of nn.Module): List of teacher models.\n",
        "        test_loader (DataLoader): DataLoader for the test set.\n",
        "        method (str): Method to combine predictions ('avg', 'max', 'vote').\n",
        "        temperature (float): Temperature for scaling logits before combining predictions.\n",
        "\n",
        "    Returns:\n",
        "        accuracy (float): Accuracy of the combined predictions on the test set.\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Collect logits from all teacher models\n",
        "        logits_list = []\n",
        "        for teacher in teacher_models:\n",
        "            teacher.eval()\n",
        "            with torch.no_grad():\n",
        "                #logits, _, _, _, _ = teacher(inputs)\n",
        "                logits = teacher(inputs)\n",
        "\n",
        "                # Apply temperature scaling\n",
        "                logits = logits / temperature\n",
        "                logits_list.append(logits)\n",
        "\n",
        "        # Stack logits from all teachers: shape [num_teachers, batch_size, num_classes]\n",
        "        stacked_logits = torch.stack(logits_list)\n",
        "\n",
        "        combined_logits = torch.mean(stacked_logits, dim=0)\n",
        "\n",
        "    return combined_logits\n",
        "\n",
        "def train_student_model(teachers, student, train_loader, epochs = 10, alpha=0.5, T=2.0):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "    for teacher in teachers:\n",
        "        teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train()  # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = combine_teacher_predictions(teachers, test_loader, temperature = 1.0)\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            # Soften the teacher logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = nn.KLDivLoss()(soft_prob, soft_targets) * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = alpha * soft_targets_loss + (1 - alpha) * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    return student\n",
        "'''"
      ],
      "metadata": {
        "id": "3QxUjMrIL8JM"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def multi_teacher_distillation(teacher_models,student_model,temperature=2.0, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Perform multi-teacher knowledge distillation on a student model.\n",
        "\n",
        "    Args:\n",
        "        student_model (nn.Module): The student model to be trained.\n",
        "        teacher_models (list of nn.Module): List of teacher models.\n",
        "        train_loader (DataLoader): DataLoader for the training set.\n",
        "        optimizer (Optimizer): Optimizer for training the student model.\n",
        "        criterion (Loss): Loss function (e.g., CrossEntropyLoss).\n",
        "        temperature (float): Temperature for scaling logits.\n",
        "        alpha (float): Weighting factor between the distillation loss and the original loss.\n",
        "        device (str): Device to use for training ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        float: The average training loss over the entire dataset.\n",
        "    \"\"\"\n",
        "    student_model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass through the student model\n",
        "        student_logits = student_model(inputs)\n",
        "\n",
        "        # Collect and combine logits from all teacher models\n",
        "        with torch.no_grad():\n",
        "            teacher_logits_list = []\n",
        "            for teacher in teacher_models:\n",
        "                teacher.eval()\n",
        "                teacher_logits = teacher(inputs)\n",
        "                teacher_logits = teacher_logits / temperature  # Apply temperature scaling\n",
        "                teacher_logits_list.append(teacher_logits)\n",
        "\n",
        "            # Average the logits from all teachers\n",
        "            avg_teacher_logits = torch.mean(torch.stack(teacher_logits_list), dim=0)\n",
        "\n",
        "        # Distillation loss: KL Divergence between teacher logits and student logits\n",
        "        distillation_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits / temperature, dim=1),\n",
        "            F.softmax(avg_teacher_logits / temperature, dim=1),\n",
        "            reduction='batchmean'\n",
        "        ) * (temperature ** 2)\n",
        "\n",
        "        # Original loss: CrossEntropy between student logits and ground truth labels\n",
        "        original_loss = criterion(student_logits, labels)\n",
        "\n",
        "        # Total loss: Weighted combination of original loss and distillation loss\n",
        "        loss = alpha * original_loss + (1.0 - alpha) * distillation_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    return student_model\n",
        "'''"
      ],
      "metadata": {
        "id": "5UA38MdKQim6"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def get_combined_teacher_logits(teachers, inputs, device='cpu'):\n",
        "    teacher_logits = []\n",
        "    with torch.no_grad():\n",
        "        for teacher in teachers:\n",
        "            teacher_logits.append(teacher(inputs.to(device)))\n",
        "\n",
        "    # Average the logits from the teacher models\n",
        "    combined_logits = sum(teacher_logits) / len(teacher_logits)\n",
        "    return combined_logits\n",
        "\n",
        "def train_student_with_combined_logits(teachers, student, loader, epochs=10, alpha=0.5, T=2.0, device='cpu'):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "    for teacher in teachers:\n",
        "        teacher.eval()  # Set all teacher models to evaluation mode\n",
        "    student.train()  # Student in training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get combined logits from the teacher models\n",
        "            combined_logits = get_combined_teacher_logits(teachers, inputs, device)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            # Soften the teacher and student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(combined_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = nn.KLDivLoss()(soft_prob, soft_targets) * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = alpha * soft_targets_loss + (1 - alpha) * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "'''"
      ],
      "metadata": {
        "id": "jGlREVvrVnnS"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLIh1yYcjx2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zSCb7TYoQvYH",
        "arXTt27qcqDJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}